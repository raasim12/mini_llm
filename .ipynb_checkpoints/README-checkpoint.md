Followed along with Andrej Karpathy's "Let's build GPT: from scratch, in code, spelled out" tutorial:

https://www.youtube.com/watch?v=kCc8FmEb1nY


1. Implemented the transformer architecture from scratch in PyTorch
2. Trained a small transformer based language model on the Shakespeare dataset https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
3. Total model parameters : 10 M trained on approximately 1 M character level tokens