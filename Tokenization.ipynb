{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15ab9a2-2167-4020-bd41-b0db57b908cf",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is at the heart of some of the main issues with LLMs.\n",
    "1. LLMs can not spell words.\n",
    "2. LLMs truggle with super simple string tasks like reversing a string.\n",
    "3. Why are LLMs worse at non-English languages ?\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef378b0-42bb-41f8-b10f-8c234321b7e0",
   "metadata": {},
   "source": [
    "Try out tokenization for different LLms here:\n",
    "\n",
    "https://tiktokenizer.vercel.app/?model=codellama%2FCodeLlama-70b-hf\n",
    "\n",
    "## Notes:\n",
    "1. Space is sometimes included as part of a token\n",
    "2. Digits are also tokenized.\n",
    "3. Tokenization is case sensitive.\n",
    "4. We have more data in english than in non-english, so english sentences have longer tokens due to this abundance. So a longer context can be fed into a model for a given number of context length.\n",
    "5. GPT2 is not very good with python. One of the reasons is that the spaces are treated as different tokens. We run out of the context length. GPT4 tokenizer handles python much better, as it groups together the white spaces, which was a deliberate choice made by openAI. \n",
    "6. Increasing the vocab size is not always good, since it increases the embedding table size and at the output, it affects the performance of softmax layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc9b98-cb62-4c26-97bb-3ac3ec27a18b",
   "metadata": {},
   "source": [
    "## Unicode Encoding\n",
    "- Unicode system includes almost 150, 000 characters across 161 scripts, what they look like and what integer represents them. We can access the unicode code point using the \"ord\" function in python.\n",
    "-  Why can't we use this coding natively for tokenization?\n",
    "    The vocab would be very largeThe unicode encoding keeps changing.\n",
    "    The unicode encoding keeps changing.\n",
    "- The unicode encoding is how the standard abstacted codes for characters are translated and stored as into sequences of bytes. UTF-8 (most common), UTF-16, UTF-32.\n",
    "- UTF-8 encoding takes each of the code point and translates it into a byte stream from 1 to 4 bytes (variable length encoding). UTF-32 is fixed length.\n",
    "- UTF-8 is preferred because it is backward compatible into much simpler  ASCII encoding.\n",
    "- UTF-16 and UTF-32 are somewhat wasteful as they have a lot of 0's between them.\n",
    "- We cannot use these encodings naively because although they would ensure a finite embedding table of size 256 and smaller output size, but we would run out of context length.\n",
    "- Readings: A programmers introduction to Unicode,  UTF-8 everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a33dc81-f3ef-4cb5-a50a-8c56d72226b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"f\") # We can't plug in a string  here. It expects a single character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03e1408-b8d7-4274-b7ac-4b2dc4a86da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 111]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of raw bytes of utf-8 encoding \n",
    "list((\"hello\").encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a6005-7e30-4633-8c2e-6f0debe7aa88",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding\n",
    "- Iteratively find the most common \"byte pair\" in the given sequence.\n",
    "- Replace that with a new token and add that new token to our vocabulary\n",
    "- Length of sequence decreases and vocabulary size increases\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02fb1420-097f-46ef-83a2-ab7768a6e673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "Length of text: 533\n",
      "________________\n",
      "TOKENS\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Length of tokens: 616\n",
      "________________\n"
     ]
    }
   ],
   "source": [
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "print(\"TEXT\")\n",
    "print(text)\n",
    "print(f\"Length of text: {len(text)}\")\n",
    "print(\"________________\")\n",
    "\n",
    "\n",
    "#UTF-8 Encoding \n",
    "tokens = text.encode(\"utf-8\") #raw bytes\n",
    "tokens = list(map(int, tokens))\n",
    "print(\"TOKENS\")\n",
    "print(tokens)\n",
    "print(f\"Length of tokens: {len(tokens)}\")\n",
    "print(\"________________\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f62ce20-b9a2-4cb3-8252-99d2028f2665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 32)\n"
     ]
    }
   ],
   "source": [
    "#Find the most frequent token\n",
    "def get_stats(ids):\n",
    "    token_dict = {}\n",
    "    for i in range(len(ids) - 1):\n",
    "        a = ids[i]\n",
    "        b = ids[i + 1]\n",
    "        token_dict[(a, b)] = token_dict.get((a, b), 0) + 1\n",
    "    return token_dict\n",
    "stats = get_stats(tokens)\n",
    "most_freq_token = max(stats, key = stats.get)\n",
    "print(most_freq_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdfbdefd-ff5b-49ac-9959-54adfdc53fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate over the sequence. Replace all instances of the most freq token (101, 32) by 256 (since our current vocab size is 255 \n",
    "def replace_token(tokens, freq_token, name):\n",
    "    seq = []\n",
    "    i = 0\n",
    "    while i < len(tokens) - 1:\n",
    "        \n",
    "        if (tokens[i], tokens[i+1]) == freq_token:\n",
    "            seq.append(name)\n",
    "            i += 2\n",
    "        else:\n",
    "            seq.append(tokens[i])\n",
    "            i += 1\n",
    "            \n",
    "    return seq\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30df5aa1-147b-49a8-929d-02b8dfc4dbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = replace_token(tokens, (101,32), 256)\n",
    "len(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c4579ca-f69b-461b-94e9-8dcb7b3c883f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc18ad-aedc-4c12-89cb-36c9fefbcdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
