{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bd1c6b-9e5a-405c-bf30-aa112d2a1dad",
   "metadata": {},
   "source": [
    "# Understanding Transformers \n",
    "\n",
    "We will follow along with the tutorial by Andrej karpathy to understand the transformer architecture and create an LLM on the Shakespeare dataset \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "1. We will use the Pytorch library\n",
    "2. We only work with chunks of the data at a time (batch training). The max length is suaully called block size or cntext length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae3d8fd-1cfb-4206-b808-9660744ead9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file to inspect the data \n",
    "with open(\"input.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0180b503-1b13-4a0c-9c03-56428d8e16da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cf931-eead-4958-919d-47a12cb5defe",
   "metadata": {},
   "source": [
    "### 1. Tokenize and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c504f42-487d-4b75-8f81-cdb5ef29c2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#Create a sorted list of all the unique characters in the input \"text\"\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "#Vocab size is the total length of the unique tokens or characters (in this case)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size ) #65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51b8aa9-616b-48e6-bab2-f5a73a3a58a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 46, 39, 58, 5, 57, 1, 59, 54, 12]\n"
     ]
    }
   ],
   "source": [
    "# Create an enocder and decoder for character level tokenizer \n",
    "#(OpenAI has atiktoken library and google has sentencepiece library for tokenizing)\n",
    "\n",
    "#Convert char to int dict (encode)\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "\n",
    "#Convert int to char dict (encode)\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "encode = lambda s : [stoi[i] for i in s]\n",
    "decode = lambda l : \"\".join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"What's up?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce9dcff-eaaf-4962-90ea-942a843db981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Pytorch Library, encode the dataset and save it in the form of a tensor\n",
    "import torch\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long) #saves the data in the form of a 1D tensor\n",
    "\n",
    "data.shape #torch.Size([1115394])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9b373-97a8-449a-b21e-289c576454ba",
   "metadata": {},
   "source": [
    "### 2. Train test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478ad200-4de9-4b34-aa5b-afe51885c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aside the last 10% of the data as the validation data and the remaining would be the train data. \n",
    "#We do not want memorization of this data, but rather Shakepeare like text\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58a6d9-3ed6-4d6b-92c2-799b9e4d81dd",
   "metadata": {},
   "source": [
    "### 3. Create Batches of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea42cbed-83f9-4f10-9ba6-00dd8f77d4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only work with chunks of the data at a time (batch training). The max length is suaully called block size or cntext length\n",
    "block_size = 8\n",
    "train_data[: block_size + 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8740435-3aee-48ed-9d7a-ba6853428c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target 47.\n",
      "When input is tensor([18, 47]), the target 56.\n",
      "When input is tensor([18, 47, 56]), the target 57.\n",
      "When input is tensor([18, 47, 56, 57]), the target 58.\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target 1.\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target 15.\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target 47.\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target 58.\n"
     ]
    }
   ],
   "source": [
    "#When we sample the chunk of data,they have mutliple samples packed into it.\n",
    "#In a chunk of 9 chars, 8 individual examples are packed inside \n",
    "#This enables the model to make predictions when the input is as little as one character up to when the input size is equal to block_size \n",
    "\n",
    "x = train_data[:block_size ]\n",
    "y = train_data[1:block_size + 1] # y is off-set by one, because the transformer predicts the next token\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[: t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context}, the target {target}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf9d3d2-85c7-46c6-a772-9c23b4906bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets:\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "When input is [24], the target 43.\n",
      "When input is [24, 43], the target 58.\n",
      "When input is [24, 43, 58], the target 5.\n",
      "When input is [24, 43, 58, 5], the target 57.\n",
      "When input is [24, 43, 58, 5, 57], the target 1.\n",
      "When input is [24, 43, 58, 5, 57, 1], the target 46.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], the target 43.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target 39.\n",
      "When input is [44], the target 53.\n",
      "When input is [44, 53], the target 56.\n",
      "When input is [44, 53, 56], the target 1.\n",
      "When input is [44, 53, 56, 1], the target 58.\n",
      "When input is [44, 53, 56, 1, 58], the target 46.\n",
      "When input is [44, 53, 56, 1, 58, 46], the target 39.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], the target 58.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target 1.\n",
      "When input is [52], the target 58.\n",
      "When input is [52, 58], the target 1.\n",
      "When input is [52, 58, 1], the target 58.\n",
      "When input is [52, 58, 1, 58], the target 46.\n",
      "When input is [52, 58, 1, 58, 46], the target 39.\n",
      "When input is [52, 58, 1, 58, 46, 39], the target 58.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], the target 1.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target 46.\n",
      "When input is [25], the target 17.\n",
      "When input is [25, 17], the target 27.\n",
      "When input is [25, 17, 27], the target 10.\n",
      "When input is [25, 17, 27, 10], the target 0.\n",
      "When input is [25, 17, 27, 10, 0], the target 21.\n",
      "When input is [25, 17, 27, 10, 0, 21], the target 1.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], the target 54.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target 39.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 #Number of training sequences to run in parallel\n",
    "block_size = 8 #Max length of the sequence\n",
    "\n",
    "def get_batch(split):\n",
    "\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #Stack multiple examples in the form of a tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "\n",
    "#32 examples packed in a tensor of size batch_size X block_size\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"Inputs:\") #Input to our transformer \n",
    "print(xb)\n",
    "print(\"Targets:\")\n",
    "print(yb)\n",
    "\n",
    "\n",
    "for i in range(batch_size): #batch dimension\n",
    "    for t in range(block_size): #time dimension\n",
    "        context = xb[i,: t+1]\n",
    "        target = yb[i, t]\n",
    "        print(f\"When input is {context.tolist()}, the target {target}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6637b0-d8b0-478d-9821-a624a40e0ece",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900bf3d0-3dbe-4625-9387-a943543d1f50",
   "metadata": {},
   "source": [
    "### i. Bigram Model\n",
    "The embedding matrix in the bigram model in a vocab_size * vocab_size matrix. For a given input, it simply plucks out the corresponding row from the embedding table. The values in that row corresponds to the probabilities of each of the vocab element to come next.\n",
    "\n",
    "The predictions are based solely on the current token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1932ffa-bdb2-4e23-9a8e-06d6c67bec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        #Each token reads off the logits for the next token from a look up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) \n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "\n",
    "        #idx and targets, both are (B,C) tensors. \n",
    "        #For each of these tokens, we will get a vocab_size \"C\" logit, and the output shape would be (B, T, C) \n",
    "        logits = self.token_embedding_table(idx) #output shape: (B, T, C)\n",
    "\n",
    "        if targets==None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "        #Pytorch expects the input for this loss function to be of shape (B, C, T), so we need to reshape our logit matrix\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)    # -log likelihood loss\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    #Generate new text\n",
    "    #This function takes in the entire context, even though the bigram model only needs the last index for th enext prediction. \n",
    "    #We do this so that this function can be kept constant for more complicated models.\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            #Get the predictions\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            # Outputs (B, T, C) Pluck the last entry in the T dimension for the next prediction\n",
    "            logits = logits[:,-1, :] #(B, C)\n",
    "\n",
    "            #Apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1) #(B, C)\n",
    "            \n",
    "            #sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) #(B, 1)\n",
    "            \n",
    "            #Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T+1)\n",
    "            \n",
    "        return idx\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(loss)  \n",
    "start = torch.zeros((1,1), dtype = torch.long)\n",
    "print(decode(model.generate(start, max_new_tokens = 100)[0] .tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6315410-9ab6-47c2-b539-c0373f61072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pytorch optimizer to train the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) #Good learning rate for larger models can be 3e-4 etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401a26e-5621-4579-a488-3c5f2d2a5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_curve = []\n",
    "\n",
    "batch_size = 32\n",
    "n_steps = 10000 #Train for 10,000 steps\n",
    "\n",
    "for _ in range(n_steps):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss_curve.append(loss.item())\n",
    "\n",
    "\n",
    "plt.plot(train_loss_curve)\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Training loss of the Bigram Model\")\n",
    "\n",
    "print(loss.item())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689bf6f-678a-4e56-b33b-6e3973095d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate from the trained model\n",
    "start = torch.zeros((1,1), dtype = torch.long)\n",
    "print(decode(model.generate(start, max_new_tokens = 500)[0] .tolist()))\n",
    "\n",
    "#Great improvement from the untrained model, but we can do better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0fb28a-e03a-4567-b59c-7ba3f7740d61",
   "metadata": {},
   "source": [
    "### The Mathematical Trick in Self-Attention\n",
    "\n",
    "We would like the tokens to interact with each other. But this communication should be one-way\n",
    "Information only flows from previous to the current time-step. No information flows from the future, because we want to predict the future.\n",
    "One way is to average the C dimension of all the tokens in context.\n",
    "This averaging or summation would be extremely lossy because we have lost all information about the relative location of all the tokens.\n",
    "Â \n",
    "We use the term Bag of words (BOW) when we are averaging a group of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba554d0-0559-4d2b-b36b-99a400be0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a toy example\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a104c-d572-450f-b66c-9715835e9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 \n",
    "\n",
    "xbow  = torch.zeros(B, T, C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] #(t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bbe9fe-a409-46c2-a810-821128303556",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / a.sum(1, keepdim = True)\n",
    "b = torch.randint(0, 10, (3,2)).float() # If we do not convert to float, it will give a runtime error\n",
    "c = a @ b\n",
    "print(f\"a : {a}\")\n",
    "print(f\"b : {b}\")\n",
    "print(f\"c : {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cde70b-158f-4799-bbd9-94d25096bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 2 \n",
    "#Converting cell 1 to matrix operation\n",
    "w = torch.tril(torch.ones(T, T))\n",
    "w = w / w.sum(1, keepdim = True)\n",
    "# Pytorch will see that the dimenisons do not match and will create a batch dimension\n",
    "xbow2 = w @ x # (B, T, T) * ( B, T, C)  ---> (B, T, C)\n",
    "torch.allclose(xbow, xbow2) #Both are equal. \n",
    "#This is a weighted sum according to the weight matrix w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac92c8-1403-4062-b707-57af6418a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3\n",
    "w = torch.zeros((T, T))\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "w = w.masked_fill(tril == 0, float('-inf'))\n",
    "w = F.softmax(w, dim = -1)\n",
    "xbow3 = w @ x\n",
    "torch.allclose(xbow3, xbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4bf09d1-886c-4640-bd0e-f5d38534aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 0, train_loss: 4.4801, val_loss: 4.4801\n",
      "Iteration number: 300, train_loss: 2.8827, val_loss: 2.9059\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/raasim/Desktop/ramsha_work/BNL/python_envs/climate_model/mini_llm/bigram.py\", line 180, in <module>\n",
      "    print(decode(model.generate(context, max_new_tokens = 500)[0] .tolist()))\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasim/Desktop/ramsha_work/BNL/python_envs/climate_model/mini_llm/bigram.py\", line 121, in generate\n",
      "    logits, loss = self(idx)\n",
      "                   ^^^^^^^^^\n",
      "  File \"/Users/raasim/Desktop/ramsha_work/BNL/python_envs/climate_model/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasim/Desktop/ramsha_work/BNL/python_envs/climate_model/mini_llm/bigram.py\", line 95, in forward\n",
      "    pos_emb = self.pos_embedding_table(torch.arange(T, device = device)) # (T, C)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasim/Desktop/ramsha_work/BNL/python_envs/climate_model/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasim/Desktop/ramsha_work/BNL/python_envs/climate_model/lib/python3.11/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n",
      "    return F.embedding(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/raasim/Desktop/ramsha_work/BNL/python_envs/climate_model/lib/python3.11/site-packages/torch/nn/functional.py\", line 2210, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: index out of range in self\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.208585023880005"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "!python bigram.py\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8acea68-9c9a-4540-a1a1-af470d4e2623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41830f03-b44f-47a6-93f7-ce0ac934c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e5b94-b5c3-4705-adcc-8b393366552f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
